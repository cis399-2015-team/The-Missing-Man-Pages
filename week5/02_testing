When one of the back-end servers is disabled, the load balancer sends all connections to the other one. When visting the domain of the load balancer, this is made obvious by the PHP code in the index.php page.

Shutting down BOTH back-end servers immediately results in unserved requests.
Once 2 (the unhealthy threshold) healthchecks fail on an instance, it is deemed "OutOfService" by the AWS console.
Visiting the domain of the ELB after the unhealthy threshold has been surpassed on both instances results in a blank page.

When an instance restarts the http daemon service it takes a few minutes for the load balancer to recognize the server as healthy and begin routing requests to it.
This is because the ELB needs to hit the healthy threshold (10 successful health checks) before it begins routing traffic to the server again.
When the server is recognized and back in service, http requests are once again served by that instance through the load balancer, as evident from access logs.
When both servers come back on, http requests are split between instances as normal.
This is evident from the access logs for the lighttpd servers.

Conclusions:
Since we have a lightweight page with fairly shallow healthchecks (only checks for index), it may be appropriate to
make health checks more frequent or to lower the healthy threshold so that servers come back online more quickly.
